{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom transformers import AutoTokenizer\n\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T18:17:37.992078Z","iopub.execute_input":"2024-04-11T18:17:37.992916Z","iopub.status.idle":"2024-04-11T18:17:44.941711Z","shell.execute_reply.started":"2024-04-11T18:17:37.992881Z","shell.execute_reply":"2024-04-11T18:17:44.939727Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# set seeds for reproducibility\nSEED = 123\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:44.943173Z","iopub.execute_input":"2024-04-11T18:17:44.943597Z","iopub.status.idle":"2024-04-11T18:17:44.951369Z","shell.execute_reply.started":"2024-04-11T18:17:44.943572Z","shell.execute_reply":"2024-04-11T18:17:44.950403Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# book collection\nBOOKS = {\n    'TheOdyssey': \"https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/TheOdyssey_Homer.txt\",\n    'PrideAndPrejudice': \"https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/PrideAndPrejudice_JaneAusten.txt\",\n    'AJourneyToTheCentreOfTheEarth': \"https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/JulesVerne_AJourneyToTheCentreOfTheEarth.txt\",\n}\nBOOKS","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:44.952342Z","iopub.execute_input":"2024-04-11T18:17:44.952636Z","iopub.status.idle":"2024-04-11T18:17:44.965188Z","shell.execute_reply.started":"2024-04-11T18:17:44.952609Z","shell.execute_reply":"2024-04-11T18:17:44.964355Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'TheOdyssey': 'https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/TheOdyssey_Homer.txt',\n 'PrideAndPrejudice': 'https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/PrideAndPrejudice_JaneAusten.txt',\n 'AJourneyToTheCentreOfTheEarth': 'https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/JulesVerne_AJourneyToTheCentreOfTheEarth.txt'}"},"metadata":{}}]},{"cell_type":"code","source":"# choose a book\nBOOK = 'PrideAndPrejudice'\n\n# get the book's url \nbook_url = BOOKS[BOOK]\n# extract the book's filename from the url\nbook_filename = os.path.basename(book_url) \n\n# download the book in local if it doesn't already exist\nif not os.path.exists(book_filename):\n    try:\n        !wget $book_url\n        print(\"File is succesfully downloaded.\")\n    except Exception as e:\n        print(f\"Could not download the book from {book_url}\")\n        print(e)\nelse:\n    print(\"File has already been downloaded.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:44.967598Z","iopub.execute_input":"2024-04-11T18:17:44.968181Z","iopub.status.idle":"2024-04-11T18:17:46.206639Z","shell.execute_reply.started":"2024-04-11T18:17:44.968156Z","shell.execute_reply":"2024-04-11T18:17:46.205511Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-04-11 18:17:45--  https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/PrideAndPrejudice_JaneAusten.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 703907 (687K) [text/plain]\nSaving to: 'PrideAndPrejudice_JaneAusten.txt'\n\nPrideAndPrejudice_J 100%[===================>] 687.41K  --.-KB/s    in 0.04s   \n\n2024-04-11 18:17:46 (18.5 MB/s) - 'PrideAndPrejudice_JaneAusten.txt' saved [703907/703907]\n\nFile is succesfully downloaded.\n","output_type":"stream"}]},{"cell_type":"code","source":"# define hyperparameters\nN_EPOCHS = 30\nBATCH_SIZE = 128\nSEQ_LEN = 200\nLEARNING_RATE = 1e-4\nEMBED_SIZE = 128\nN_HEADS = 2\nNUM_LAYERS = 1\nBATCH_FIRST=True\nNORM_FIRST=False\nTEST_SIZE = 0.2\nEARLY_STOPPING_STEP_SIZE = 5\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBEST_MODEL_FILEPATH = 'best_model.pt'\n\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:46.208454Z","iopub.execute_input":"2024-04-11T18:17:46.208879Z","iopub.status.idle":"2024-04-11T18:17:46.245368Z","shell.execute_reply.started":"2024-04-11T18:17:46.208835Z","shell.execute_reply":"2024-04-11T18:17:46.244308Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"def load_text(fp):\n    with open(fp, mode='r') as infile:\n        text = ''.join([row for row in infile])\n    return text\n\n# load text\ntext = load_text(book_filename)\n\n# Get the number of characters in the text\ntext_len = len(text)\nprint(f'Number of characters in the text = {text_len}')\nprint(\"--------------\\n\")\n\n# Print a sample from the text\nidx = np.random.randint(low=0, high=text_len-SEQ_LEN)\nprint(f'Sample text:\\n{text[idx:(idx + SEQ_LEN)]}')\nprint(\"--------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:46.246666Z","iopub.execute_input":"2024-04-11T18:17:46.247022Z","iopub.status.idle":"2024-04-11T18:17:46.260918Z","shell.execute_reply.started":"2024-04-11T18:17:46.246979Z","shell.execute_reply":"2024-04-11T18:17:46.259960Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of characters in the text = 690654\n--------------\n\nSample text:\n it\nwas all done very well. She had also to anticipate how her visit would\npass, the quiet tenor of their usual employments, the vexatious\ninterruptions of Mr. Collins, and the gaieties of their inter\n--------------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:46.262301Z","iopub.execute_input":"2024-04-11T18:17:46.262613Z","iopub.status.idle":"2024-04-11T18:17:47.864429Z","shell.execute_reply.started":"2024-04-11T18:17:46.262588Z","shell.execute_reply":"2024-04-11T18:17:47.863631Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660ec26cc5bb4b67a07c892c2b61d031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f9a7d4a7c946b48f55bd9404596ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b202e739cbe040a69cfabac9957b8dd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a10ebdf5ff14212830d8f7bf35e77e9"}},"metadata":{}}]},{"cell_type":"code","source":"# tokenize the whole book\ntokens = tokenizer.tokenize(text)\n# get the number of tokens\nn_tokens = len(tokens)\nprint(f\"There are {n_tokens} tokens in the text.\")\n\n# set vocabulary (set of unique tokens in the source text)\n#vocab = sorted(list(set(tokens)))\n# get the vocabulary size\nvocab_size = len(tokenizer)\nprint(f\"Vocabulary size = {vocab_size}\")\nprint(\"--------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:47.865585Z","iopub.execute_input":"2024-04-11T18:17:47.865887Z","iopub.status.idle":"2024-04-11T18:17:48.466114Z","shell.execute_reply.started":"2024-04-11T18:17:47.865863Z","shell.execute_reply":"2024-04-11T18:17:48.465134Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (159555 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"There are 159555 tokens in the text.\nVocabulary size = 28996\n--------------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# define the train and test data\ntest_len = int(n_tokens * TEST_SIZE)\ntrain_data = tokens[:-test_len]\ntest_data = tokens[-test_len:]\n\n# print the length of train and test data\nprint(f'Length of the train text = {len(train_data)}')\nprint(f'Length of the test text  = {len(test_data)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.467227Z","iopub.execute_input":"2024-04-11T18:17:48.467543Z","iopub.status.idle":"2024-04-11T18:17:48.473936Z","shell.execute_reply.started":"2024-04-11T18:17:48.467518Z","shell.execute_reply":"2024-04-11T18:17:48.473005Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Length of the train text = 127644\nLength of the test text  = 31911\n","output_type":"stream"}]},{"cell_type":"code","source":"# define the TextDataset as an instance of torch.nn.Dataset\nclass TextDataset(Dataset):\n    def __init__(self, text, seq_len):\n        super().__init__()\n        self.text = text\n        self.seq_len = seq_len\n\n    def __len__(self):\n        # define the number of samples in the dataset\n        return len(self.text) - self.seq_len - 1\n\n    def __getitem__(self, idx):\n        # x = [c0, c1, ... cN]\n        x = torch.tensor(\n            [tokenizer.convert_tokens_to_ids(token) for token in self.text[idx:(idx + self.seq_len)]], \n            dtype=torch.long\n        )\n        # y = [c1, c1, ... c(N+1)]\n        y = torch.tensor(\n            [tokenizer.convert_tokens_to_ids(token) for token in self.text[(idx + 1):(idx + 1 + self.seq_len)]], \n            dtype=torch.long\n        )\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.476613Z","iopub.execute_input":"2024-04-11T18:17:48.476871Z","iopub.status.idle":"2024-04-11T18:17:48.487413Z","shell.execute_reply.started":"2024-04-11T18:17:48.476850Z","shell.execute_reply":"2024-04-11T18:17:48.486438Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# initialize train and test datasets\ntrain_dset = TextDataset(train_data, SEQ_LEN)\ntest_dset = TextDataset(test_data, SEQ_LEN)\n\n# initialize train and test iterators\ntrain_iterator = DataLoader(train_dset, BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\ntest_iterator = DataLoader(test_dset, BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.488396Z","iopub.execute_input":"2024-04-11T18:17:48.488642Z","iopub.status.idle":"2024-04-11T18:17:48.502003Z","shell.execute_reply.started":"2024-04-11T18:17:48.488616Z","shell.execute_reply":"2024-04-11T18:17:48.501187Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# define GPT network\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, embed_size, seq_len, num_layers, n_heads, batch_first, norm_first):\n        super().__init__()\n        self.seq_len = seq_len\n        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n        self.positional_embedding = nn.Embedding(seq_len, embed_size)\n        # define transformer encoder network\n        encoder_layer = nn.TransformerEncoderLayer(embed_size, N_HEADS, batch_first=batch_first, norm_first=norm_first)\n        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers)\n        self.fc = nn.Linear(embed_size, vocab_size)\n            \n    def forward(self, x, is_causal=False):\n        word_embed = self.word_embedding(x)\n        pos_embed = self.positional_embedding(torch.arange(x.shape[1], dtype=torch.long).to(x.device))        \n        x = word_embed + pos_embed\n        if is_causal:\n            mask = nn.Transformer.generate_square_subsequent_mask(self.seq_len).to(x.device)\n        else: mask=None\n        x = self.encoder(x, mask=mask, is_causal=is_causal)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.503089Z","iopub.execute_input":"2024-04-11T18:17:48.503376Z","iopub.status.idle":"2024-04-11T18:17:48.513434Z","shell.execute_reply.started":"2024-04-11T18:17:48.503354Z","shell.execute_reply":"2024-04-11T18:17:48.512548Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def compute_loss(logits, y):\n    # get the logits dimensions for better readibility and understandibility\n    B, S, C = logits.shape\n    # compute loss\n    loss = F.cross_entropy(logits.view(B*S, C), y.view(-1))\n    return loss\n\ndef batch_loop(model, x, y, is_causal=False):\n    # forward pass\n    logits = model(x, is_causal=is_causal)\n    # compute loss\n    loss = compute_loss(logits, y)\n    return loss\n\ndef train_epoch(model, iterator):\n    # global variable\n    global DEVICE\n    # set model to training mode\n    model.train()\n    # initialize epoch loss\n    loss_epoch = 0.0\n    \n    # iterate over batches\n    for iter_idx, (x, y) in tqdm(enumerate(iterator), total=len(iterator), desc=\"Training Progress\"):\n        # carry tensors to available device\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        # train batch and compute loss\n        loss = batch_loop(model, x, y, is_causal=True)        \n        # gradient update\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # add batch loss to epoch loss sum\n        loss_epoch += loss.item()\n        \n    # get the number of batches\n    n_batch = len(iterator)\n    # compute average loss\n    loss_epoch_avg = loss_epoch / n_batch\n    return loss_epoch_avg\n\ndef evaluate(model, iterator):\n    # global variable\n    global DEVICE\n    # set model to evaluation mode\n    model.eval()\n    # initialize loss\n    loss_sum = 0.0\n    \n    with torch.inference_mode():\n        # iterate over batches\n        for iter_idx, (x, y) in tqdm(enumerate(iterator), total=len(iterator), desc=\"Evaluation Progress\"):\n            # carry tensors to available device\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            # train batch and compute loss\n            loss = batch_loop(model, x, y)        \n            # add batch loss to epoch loss sum\n            loss_sum += loss.item()\n        \n    # get the number of batches\n    n_batch = len(iterator)\n    # compute average loss\n    loss_avg = loss_sum / n_batch\n    return loss_avg","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.514959Z","iopub.execute_input":"2024-04-11T18:17:48.515450Z","iopub.status.idle":"2024-04-11T18:17:48.528273Z","shell.execute_reply.started":"2024-04-11T18:17:48.515418Z","shell.execute_reply":"2024-04-11T18:17:48.527389Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def generate(model, text, max_length=100, temp=1.0):\n    # global variables\n    global DEVICE, tokenizer\n    # set model to evaluation mode\n    model.eval()\n    input_token_ids = [tokenizer.convert_tokens_to_ids(word) for word in tokenizer.tokenize(text)]\n    tokens = torch.tensor([input_token_ids], dtype=torch.long)\n    generated_token_ids = input_token_ids\n    with torch.inference_mode():\n        for _ in range(max_length):\n            logits = model(tokens)\n            next_token_logits = logits[:,-1,:]\n            next_token_id = torch.multinomial(next_token_logits.div(temp).exp(), num_samples=1)\n            tokens = torch.cat((tokens, torch.tensor([[next_token_id]])), dim=1)\n            generated_token_ids.append(next_token_id.item())\n    \n    return generated_token_ids","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.529350Z","iopub.execute_input":"2024-04-11T18:17:48.529825Z","iopub.status.idle":"2024-04-11T18:17:48.541999Z","shell.execute_reply.started":"2024-04-11T18:17:48.529791Z","shell.execute_reply":"2024-04-11T18:17:48.541231Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# initialize GPT model\nmodel = GPT(\n    vocab_size=vocab_size,\n    embed_size=EMBED_SIZE,\n    seq_len=SEQ_LEN,\n    num_layers=NUM_LAYERS,\n    n_heads=N_HEADS,\n    batch_first=BATCH_FIRST,\n    norm_first=NORM_FIRST,\n).to(DEVICE)\nmodel = torch.nn.DataParallel(model)\n\n# initialize optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\nbest_test_loss = float('inf')\nbest_loss_streak = 0\n\n# iterate over epochs\nfor epoch_idx in range(N_EPOCHS):\n    train_loss = train_epoch(model, train_iterator)\n    test_loss = evaluate(model, test_iterator)\n    print(f\"Epoch {epoch_idx:02} - Train Loss = {train_loss:.3f}\\tTest Loss = {test_loss:.3f}\")\n    \n    # save the current model as the best model if the current test loss is the least achieved \n    if test_loss < best_test_loss:\n        # save the curent model's parameters as the best model parameters\n        torch.save(model.state_dict(), BEST_MODEL_FILEPATH)\n        # replace the best test loss with the current best loss\n        best_test_loss = test_loss\n        # reset early stoppping counter \n        best_streak_count = 0\n        # display info\n        print(f'The best model is found and saved. Current best test loss = {best_test_loss:.3f}')\n        text = \"A\"\n        generated_tokens = generate(model, text)\n        print(tokenizer.decode(generated_tokens))\n    else:\n        # update early stoppping counter \n        best_streak_count += 1\n\n    # check early stopping condition\n    if best_streak_count == EARLY_STOPPING_STEP_SIZE:\n        print(f\"A better model has not been found in the last {EARLY_STOPPING_STEP_SIZE} epochs. Early stopping...\")\n        break\n        \n    print(\"--------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:30:36.665149Z","iopub.execute_input":"2024-04-11T18:30:36.666107Z","iopub.status.idle":"2024-04-11T19:06:26.465887Z","shell.execute_reply.started":"2024-04-11T18:30:36.666076Z","shell.execute_reply":"2024-04-11T19:06:26.464899Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:57<00:00,  4.19it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:32<00:00,  7.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 00 - Train Loss = 6.182\tTest Loss = 5.154\nThe best model is found and saved. Current best test loss = 5.154\n","output_type":"stream"},{"name":"stderr","text":"2024-04-11 18:35:08.567079: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 18:35:08.567190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 18:35:08.684102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"A tags place, Camera him, and considered Mr. they - - but aloud? Bennet ; and Mr. Aftercy of Mr. Collins? but What _zad will remained, and what to any one of equal evergreen was any thing Jane case ; by his friends, it is Mrs. But thaton, that. But her for them Miss Bingley! youju I dare too, Elizabeth, as speaking to make \" \" it in ladies, and that it had been at four. Goldman must\n---------------\n\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:55<00:00,  4.23it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 - Train Loss = 4.680\tTest Loss = 4.768\nThe best model is found and saved. Current best test loss = 4.768\nA Novel. Philips, and welcomed for the e time, was expected in silent away to his arrival of two on arguments, which the girls to make them are longnt, she in behalf by - - - on breathless himself to involve of each question, at least was agreed cannot general countenance. On on whose manner as an excellent are severe few days, \" That is a manner from her into the honour of sinking to your surrounding! denying great deal for her. Gardiner ; shebred proved her\n---------------\n\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:55<00:00,  4.23it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 - Train Loss = 4.228\tTest Loss = 4.703\nThe best model is found and saved. Current best test loss = 4.703\nA younger excellent water of his cousin, however, it ; and that they were attempted to the endeavour to be, in this time to have been travelling that ever motive, whim impertinentmost stepr. She blushed. game from Londoneneds. In know and in all the right, and the garden, firm, it a good at her sister, and supertivat future feelings, when she had passed between the punctulate her. She was a clever - corner\n---------------\n\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:55<00:00,  4.22it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 - Train Loss = 3.975\tTest Loss = 4.719\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:55<00:00,  4.22it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 - Train Loss = 3.796\tTest Loss = 4.768\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:56<00:00,  4.21it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 - Train Loss = 3.658\tTest Loss = 4.837\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:56<00:00,  4.21it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 - Train Loss = 3.538\tTest Loss = 4.927\n--------------\n\n","output_type":"stream"},{"name":"stderr","text":"Training Progress: 100%|██████████| 995/995 [03:54<00:00,  4.24it/s]\nEvaluation Progress: 100%|██████████| 248/248 [00:31<00:00,  7.95it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 07 - Train Loss = 3.419\tTest Loss = 5.039\nA better model has not been found in the last 5 epochs. Early stopping...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}