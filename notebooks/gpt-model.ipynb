{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nfrom transformers import AutoTokenizer\n\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T18:17:37.992078Z","iopub.execute_input":"2024-04-11T18:17:37.992916Z","iopub.status.idle":"2024-04-11T18:17:44.941711Z","shell.execute_reply.started":"2024-04-11T18:17:37.992881Z","shell.execute_reply":"2024-04-11T18:17:44.939727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set seeds for reproducibility\nSEED = 123\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:44.943173Z","iopub.execute_input":"2024-04-11T18:17:44.943597Z","iopub.status.idle":"2024-04-11T18:17:44.951369Z","shell.execute_reply.started":"2024-04-11T18:17:44.943572Z","shell.execute_reply":"2024-04-11T18:17:44.950403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book collection\nBOOKS = {\n    'TheOdyssey': \"https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/TheOdyssey_Homer.txt\",\n    'PrideAndPrejudice': \"https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/PrideAndPrejudice_JaneAusten.txt\",\n    'AJourneyToTheCentreOfTheEarth': \"https://raw.githubusercontent.com/cetinsamet/data-science/main/data/book/JulesVerne_AJourneyToTheCentreOfTheEarth.txt\",\n}\nBOOKS","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:44.952342Z","iopub.execute_input":"2024-04-11T18:17:44.952636Z","iopub.status.idle":"2024-04-11T18:17:44.965188Z","shell.execute_reply.started":"2024-04-11T18:17:44.952609Z","shell.execute_reply":"2024-04-11T18:17:44.964355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose a book\nBOOK = 'PrideAndPrejudice'\n\n# get the book's url \nbook_url = BOOKS[BOOK]\n# extract the book's filename from the url\nbook_filename = os.path.basename(book_url) \n\n# download the book in local if it doesn't already exist\nif not os.path.exists(book_filename):\n    try:\n        !wget $book_url\n        print(\"File is succesfully downloaded.\")\n    except Exception as e:\n        print(f\"Could not download the book from {book_url}\")\n        print(e)\nelse:\n    print(\"File has already been downloaded.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:44.967598Z","iopub.execute_input":"2024-04-11T18:17:44.968181Z","iopub.status.idle":"2024-04-11T18:17:46.206639Z","shell.execute_reply.started":"2024-04-11T18:17:44.968156Z","shell.execute_reply":"2024-04-11T18:17:46.205511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define hyperparameters\nN_EPOCHS = 30\nBATCH_SIZE = 128\nSEQ_LEN = 200\nLEARNING_RATE = 1e-4\nEMBED_SIZE = 128\nN_HEADS = 2\nNUM_LAYERS = 1\nBATCH_FIRST=True\nNORM_FIRST=False\nTEST_SIZE = 0.2\nEARLY_STOPPING_STEP_SIZE = 5\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBEST_MODEL_FILEPATH = 'best_model.pt'\n\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:46.208454Z","iopub.execute_input":"2024-04-11T18:17:46.208879Z","iopub.status.idle":"2024-04-11T18:17:46.245368Z","shell.execute_reply.started":"2024-04-11T18:17:46.208835Z","shell.execute_reply":"2024-04-11T18:17:46.244308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_text(fp):\n    with open(fp, mode='r') as infile:\n        text = ''.join([row for row in infile])\n    return text\n\n# load text\ntext = load_text(book_filename)\n\n# Get the number of characters in the text\ntext_len = len(text)\nprint(f'Number of characters in the text = {text_len}')\nprint(\"--------------\\n\")\n\n# Print a sample from the text\nidx = np.random.randint(low=0, high=text_len-SEQ_LEN)\nprint(f'Sample text:\\n{text[idx:(idx + SEQ_LEN)]}')\nprint(\"--------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:46.246666Z","iopub.execute_input":"2024-04-11T18:17:46.247022Z","iopub.status.idle":"2024-04-11T18:17:46.260918Z","shell.execute_reply.started":"2024-04-11T18:17:46.246979Z","shell.execute_reply":"2024-04-11T18:17:46.259960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:46.262301Z","iopub.execute_input":"2024-04-11T18:17:46.262613Z","iopub.status.idle":"2024-04-11T18:17:47.864429Z","shell.execute_reply.started":"2024-04-11T18:17:46.262588Z","shell.execute_reply":"2024-04-11T18:17:47.863631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize the whole book\ntokens = tokenizer.tokenize(text)\n# get the number of tokens\nn_tokens = len(tokens)\nprint(f\"There are {n_tokens} tokens in the text.\")\n\n# set vocabulary (set of unique tokens in the source text)\n#vocab = sorted(list(set(tokens)))\n# get the vocabulary size\nvocab_size = len(tokenizer)\nprint(f\"Vocabulary size = {vocab_size}\")\nprint(\"--------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:47.865585Z","iopub.execute_input":"2024-04-11T18:17:47.865887Z","iopub.status.idle":"2024-04-11T18:17:48.466114Z","shell.execute_reply.started":"2024-04-11T18:17:47.865863Z","shell.execute_reply":"2024-04-11T18:17:48.465134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the train and test data\ntest_len = int(n_tokens * TEST_SIZE)\ntrain_data = tokens[:-test_len]\ntest_data = tokens[-test_len:]\n\n# print the length of train and test data\nprint(f'Length of the train text = {len(train_data)}')\nprint(f'Length of the test text  = {len(test_data)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.467227Z","iopub.execute_input":"2024-04-11T18:17:48.467543Z","iopub.status.idle":"2024-04-11T18:17:48.473936Z","shell.execute_reply.started":"2024-04-11T18:17:48.467518Z","shell.execute_reply":"2024-04-11T18:17:48.473005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the TextDataset as an instance of torch.nn.Dataset\nclass TextDataset(Dataset):\n    def __init__(self, text, seq_len):\n        super().__init__()\n        self.text = text\n        self.seq_len = seq_len\n\n    def __len__(self):\n        # define the number of samples in the dataset\n        return len(self.text) - self.seq_len - 1\n\n    def __getitem__(self, idx):\n        # x = [c0, c1, ... cN]\n        x = torch.tensor(\n            [tokenizer.convert_tokens_to_ids(token) for token in self.text[idx:(idx + self.seq_len)]], \n            dtype=torch.long\n        )\n        # y = [c1, c1, ... c(N+1)]\n        y = torch.tensor(\n            [tokenizer.convert_tokens_to_ids(token) for token in self.text[(idx + 1):(idx + 1 + self.seq_len)]], \n            dtype=torch.long\n        )\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.476613Z","iopub.execute_input":"2024-04-11T18:17:48.476871Z","iopub.status.idle":"2024-04-11T18:17:48.487413Z","shell.execute_reply.started":"2024-04-11T18:17:48.476850Z","shell.execute_reply":"2024-04-11T18:17:48.486438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize train and test datasets\ntrain_dset = TextDataset(train_data, SEQ_LEN)\ntest_dset = TextDataset(test_data, SEQ_LEN)\n\n# initialize train and test iterators\ntrain_iterator = DataLoader(train_dset, BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\ntest_iterator = DataLoader(test_dset, BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.488396Z","iopub.execute_input":"2024-04-11T18:17:48.488642Z","iopub.status.idle":"2024-04-11T18:17:48.502003Z","shell.execute_reply.started":"2024-04-11T18:17:48.488616Z","shell.execute_reply":"2024-04-11T18:17:48.501187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define GPT network\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, embed_size, seq_len, num_layers, n_heads, batch_first, norm_first):\n        super().__init__()\n        self.seq_len = seq_len\n        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n        self.positional_embedding = nn.Embedding(seq_len, embed_size)\n        # define transformer encoder network\n        encoder_layer = nn.TransformerEncoderLayer(embed_size, N_HEADS, batch_first=batch_first, norm_first=norm_first)\n        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers)\n        self.fc = nn.Linear(embed_size, vocab_size)\n            \n    def forward(self, x, is_causal=False):\n        word_embed = self.word_embedding(x)\n        pos_embed = self.positional_embedding(torch.arange(x.shape[1], dtype=torch.long).to(x.device))        \n        x = word_embed + pos_embed\n        if is_causal:\n            mask = nn.Transformer.generate_square_subsequent_mask(self.seq_len).to(x.device)\n        else: mask=None\n        x = self.encoder(x, mask=mask, is_causal=is_causal)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.503089Z","iopub.execute_input":"2024-04-11T18:17:48.503376Z","iopub.status.idle":"2024-04-11T18:17:48.513434Z","shell.execute_reply.started":"2024-04-11T18:17:48.503354Z","shell.execute_reply":"2024-04-11T18:17:48.512548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_loss(logits, y):\n    # get the logits dimensions for better readibility and understandibility\n    B, S, C = logits.shape\n    # compute loss\n    loss = F.cross_entropy(logits.view(B*S, C), y.view(-1))\n    return loss\n\ndef batch_loop(model, x, y, is_causal=False):\n    # forward pass\n    logits = model(x, is_causal=is_causal)\n    # compute loss\n    loss = compute_loss(logits, y)\n    return loss\n\ndef train_epoch(model, iterator):\n    # global variable\n    global DEVICE\n    # set model to training mode\n    model.train()\n    # initialize epoch loss\n    loss_epoch = 0.0\n    \n    # iterate over batches\n    for iter_idx, (x, y) in tqdm(enumerate(iterator), total=len(iterator), desc=\"Training Progress\"):\n        # carry tensors to available device\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        # train batch and compute loss\n        loss = batch_loop(model, x, y, is_causal=True)        \n        # gradient update\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # add batch loss to epoch loss sum\n        loss_epoch += loss.item()\n        \n    # get the number of batches\n    n_batch = len(iterator)\n    # compute average loss\n    loss_epoch_avg = loss_epoch / n_batch\n    return loss_epoch_avg\n\ndef evaluate(model, iterator):\n    # global variable\n    global DEVICE\n    # set model to evaluation mode\n    model.eval()\n    # initialize loss\n    loss_sum = 0.0\n    \n    with torch.inference_mode():\n        # iterate over batches\n        for iter_idx, (x, y) in tqdm(enumerate(iterator), total=len(iterator), desc=\"Evaluation Progress\"):\n            # carry tensors to available device\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            # train batch and compute loss\n            loss = batch_loop(model, x, y)        \n            # add batch loss to epoch loss sum\n            loss_sum += loss.item()\n        \n    # get the number of batches\n    n_batch = len(iterator)\n    # compute average loss\n    loss_avg = loss_sum / n_batch\n    return loss_avg","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.514959Z","iopub.execute_input":"2024-04-11T18:17:48.515450Z","iopub.status.idle":"2024-04-11T18:17:48.528273Z","shell.execute_reply.started":"2024-04-11T18:17:48.515418Z","shell.execute_reply":"2024-04-11T18:17:48.527389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(model, text, max_length=100, temp=1.0):\n    # global variables\n    global DEVICE, tokenizer\n    # set model to evaluation mode\n    model.eval()\n    input_token_ids = [tokenizer.convert_tokens_to_ids(word) for word in tokenizer.tokenize(text)]\n    tokens = torch.tensor([input_token_ids], dtype=torch.long)\n    generated_token_ids = input_token_ids\n    with torch.inference_mode():\n        for _ in range(max_length):\n            logits = model(tokens)\n            next_token_logits = logits[:,-1,:]\n            next_token_id = torch.multinomial(next_token_logits.div(temp).exp(), num_samples=1)\n            tokens = torch.cat((tokens, torch.tensor([[next_token_id]])), dim=1)\n            generated_token_ids.append(next_token_id.item())\n    \n    return generated_token_ids","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:17:48.529350Z","iopub.execute_input":"2024-04-11T18:17:48.529825Z","iopub.status.idle":"2024-04-11T18:17:48.541999Z","shell.execute_reply.started":"2024-04-11T18:17:48.529791Z","shell.execute_reply":"2024-04-11T18:17:48.541231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize GPT model\nmodel = GPT(\n    vocab_size=vocab_size,\n    embed_size=EMBED_SIZE,\n    seq_len=SEQ_LEN,\n    num_layers=NUM_LAYERS,\n    n_heads=N_HEADS,\n    batch_first=BATCH_FIRST,\n    norm_first=NORM_FIRST,\n).to(DEVICE)\nmodel = torch.nn.DataParallel(model)\n\n# initialize optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\nbest_test_loss = float('inf')\nbest_loss_streak = 0\n\n# iterate over epochs\nfor epoch_idx in range(N_EPOCHS):\n    train_loss = train_epoch(model, train_iterator)\n    test_loss = evaluate(model, test_iterator)\n    print(f\"Epoch {epoch_idx:02} - Train Loss = {train_loss:.3f}\\tTest Loss = {test_loss:.3f}\")\n    \n    # save the current model as the best model if the current test loss is the least achieved \n    if test_loss < best_test_loss:\n        # save the curent model's parameters as the best model parameters\n        torch.save(model.state_dict(), BEST_MODEL_FILEPATH)\n        # replace the best test loss with the current best loss\n        best_test_loss = test_loss\n        # reset early stoppping counter \n        best_streak_count = 0\n        # display info\n        print(f'The best model is found and saved. Current best test loss = {best_test_loss:.3f}')\n        text = \"A\"\n        generated_tokens = generate(model, text)\n        print(tokenizer.decode(generated_tokens))\n    else:\n        # update early stoppping counter \n        best_streak_count += 1\n\n    # check early stopping condition\n    if best_streak_count == EARLY_STOPPING_STEP_SIZE:\n        print(f\"A better model has not been found in the last {EARLY_STOPPING_STEP_SIZE} epochs. Early stopping...\")\n        break\n        \n    print(\"--------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:30:36.665149Z","iopub.execute_input":"2024-04-11T18:30:36.666107Z","iopub.status.idle":"2024-04-11T19:06:26.465887Z","shell.execute_reply.started":"2024-04-11T18:30:36.666076Z","shell.execute_reply":"2024-04-11T19:06:26.464899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}